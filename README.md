<style>
r { color: Red }
o { color: Orange }
g { color: Green }
c { color: Cyan }
blue { color: Blue }
customb { color: #006699 }
</style>

# Welcome!
I am a fourth-year Ph.D. candidate in Computer Science at the University of Illinois at Urbana-Champaign advised by Prof. Julia Hockenmaier. My research primarily centers around: 
- Improving LLMs with structured reasoning
- Enhancing LLM interpretability
- Creating and analyzing novel deep learning model architectures

## Education
- University of Illinois at Urbana-Champaign &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Aug 2021 ~ Current
  - Ph.D. student in Computer Science								       		
- Seoul National University  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Mar 2013 ~ Feb 2021
  - B.S. in Electrical and Computer Enginering

## Fellowships
- University of Illinois at Urbana-Champaign
  - CS Ph.D. Fellowship &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sep 2023 - May 2024
  - CS Ph.D. Fellowship &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sep 2024 - May 2025

## Publications
1. Anonymous <br><customb>Committed to NAACL 2025 (Meta Review Score: 4!)</customb><br><ins>Ikhyun Cho</ins>, Changyeon Park, and Julia Hockenmaier<br><br>
2. Tutor-ICL: Guiding Large Language Models for Improved In-Context Learning Performance <br><a href="https://aclanthology.org/2024.findings-emnlp.554/" style="color: #006699;">EMNLP 2024 Findings</a><br><ins>Ikhyun Cho</ins>, Gaeul Kwon, and Julia Hockenmaier<br><br>
3. VisualSiteDiary: A Detector-Free Vision-Language Transformer Model for Captioning Photologs for Daily Construction Reporting and Image Retrievals<br><a href="https://www.sciencedirect.com/science/article/pii/S092658052400219X" style="color: #006699;">Elsevier: Automation in Construction</a><br>Yoonhwa Jung, <ins>Ikhyun Cho</ins>, and Julia Hockenmaier<br><br>
4. SIR-ABSC: Incorporating Syntax into RoBERTa-based Sentiment Analysis Models with a Special Aggregator Token<br><a href="https://aclanthology.org/2023.findings-emnlp.572/" style="color: #006699;">EMNLP 2023 Findings</a><br><ins>Ikhyun Cho</ins>, Yoonhwa Jung, and Julia Hockenmaier<br><br>
5. Duplicate-and-Share: A Novel Approach to Efficient Vision Transformer Unlearning<br><customb>Under review at AAAI 2025</customb><br><ins>Ikhyun Cho</ins>, Changyeon Park, and Julia Hockenmaier<br><br>
6. Pea-KD: Parameter-efficient and accurate Knowledge Distillation on BERT<br><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0263592" style="color: #006699;">PLOS ONE 2022</a><br><ins>Ikhyun Cho</ins> and U Kang<br><br>
7. SensiMix: Sensitivity-Aware 8-bit index & 1-bit value mixed precision quantization for BERT compression<br><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0265621" style="color: #006699;">PLOS ONE 2022</a><br>Tairen Piao, <ins>Ikhyun Cho</ins>, and U Kang
