<style>
r { color: Red }
o { color: Orange }
g { color: Green }
c { color: Cyan }
blue { color: Blue }
customb { color: #006699 }
</style>

# Welcome!
I am a third-year Ph.D. student in Computer Science at the University of Illinois at Urbana-Champaign advised by Prof. Julia Hockenmaier. My research primarily centers around model architecture engineering, including: 
- Creating and analyzing novel deep learning model architectures
- Analyzing and enhancing the Mixture-of-Experts framework
- Designing user-controllable deep learning models
- Developing novel (model compression, machine unlearning, etc.) techniques.

## Education
- University of Illinois at Urbana-Champaign &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Aug 2021 ~ Current
  - Ph.D. student in Computer Science								       		
- Seoul National University  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Mar 2013 ~ Feb 2021
  - B.S. in Electrical and Computer Enginering

## Fellowships
- University of Illinois at Urbana-Champaign
  - CS Ph.D. Fellowship &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Sep 2023 - May 2024

## Publications
<!--1. Prompting for Model Compression: A Novel Transformer-based Model Compression Technique via Prompt Learning and Parameter Sharing<br><customb>In preparation (estimated date: 2.15.2024)</customb><br>**Ikhyun Cho** and Julia Hockenmaier-->
1. Duplicate-and-Share: A Novel Approach to Efficient Vision Transformer Unlearning<br><customb>Under review at KDD 2024</customb><br>**Ikhyun Cho**, Changyeon Park, and Julia Hockenmaier
2. ViT-MUL:  Baseline Study on Recent Machine Unlearning Methods Applied to Vision Transformers <br><customb>Uploaded to ArXiv on Feb. 7 2024</customb><br>**Ikhyun Cho**, Changyeon Park, and Julia Hockenmaier<br><a href="pdfs/ViTMUL (ArXiv).pdf" style="color: #006699; text-decoration: underline;text-decoration-style: dotted;">[PDF]</a>
3. Attack and Reset for Unlearning: Exploiting Adversarial Noise toward Machine Unlearning through Parameter Re-initialization<br><customb>Under review at IJCAI 2024</customb><br>Yoonhwa Jung, **Ikhyun Cho**, Shun-Hsiang Hsu, and Julia Hockenmaier<br><a href="https://arxiv.org/abs/2401.08998" style="color: #006699; text-decoration: underline;text-decoration-style: dotted;">[PDF]</a>
4. Prompting for Mixture-of-Experts: A Prompt-based Mixture-of-Experts framework for Stylized Image Captioning<br><customb>Under review at CVPR 2024</customb><br>**Ikhyun Cho**, Yoonhwa Jung, and Julia Hockenmaier
5. VisualSiteDiary: A Detector-Free Vision-Language Transformer Model for Captioning Photologs for Daily Construction Reporting and Image Retrievals<br><customb>Under review at Elsevier: Automation in Construction</customb><br>Yoonhwa Jung, **Ikhyun Cho**, and Julia Hockenmaier
6. SIR-ABSC: Incorporating Syntax into RoBERTa-based Sentiment Analysis Models with a Special Aggregator Token<br><customb>EMNLP 2023 Findings</customb><br>**Ikhyun Cho**, Yoonhwa Jung, and Julia Hockenmaier<br><a href="https://aclanthology.org/2023.findings-emnlp.572/" style="color: #006699; text-decoration: underline;text-decoration-style: dotted;">[PDF]</a>
7. Pea-KD: Parameter-efficient and accurate Knowledge Distillation on BERT<br><customb>PLOS ONE 2022</customb><br>**Ikhyun Cho** and U Kang<br><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0263592" style="color: #006699; text-decoration: underline;text-decoration-style: dotted;">[PDF]</a>
8. SensiMix: Sensitivity-Aware 8-bit index & 1-bit value mixed precision quantization for BERT compression<br><customb>PLOS ONE 2022</customb><br>Tairen Piao, **Ikhyun Cho**, and U Kang<br><a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0265621" style="color: #006699; text-decoration: underline;text-decoration-style: dotted;">[PDF]</a>
